[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository. This path must be absolute.
dags_folder = G:\Diabetes Risk Prediction System\src\airflow\dags

# Hostname by providing a path to a callable, which will be called
# without any arguments and whose return value will be used.
hostname_callable = airflow.utils.net.get_hostname

# Default timezone in case supplied date times are naive
# can be utc (default), system, or any IANA timezone string (e.g. Europe/Amsterdam)
default_timezone = utc

# The executor class that airflow should use. Choices include:
# ``SequentialExecutor``, ``LocalExecutor``, ``CeleryExecutor``, ``DaskExecutor``,
# ``KubernetesExecutor``, ``CeleryKubernetesExecutor`` or the full import path to
# the class when using a custom executor.
executor = SequentialExecutor

# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engines.
# More information on supported database engines can be found here:
# https://docs.sqlalchemy.org/en/latest/core/engines.html
#
# Examples of database connection strings:
# sqlite:////tmp/airflow.db
# postgresql+psycopg2://airflow:[email protected]/airflow
# mysql://airflow:[email protected]/airflow
# redis://redis-host:6379/0
# rabbitmq://rabbitmq-host/vhost
sql_alchemy_conn = sqlite:///G:/Diabetes Risk Prediction System/src/airflow/airflow.db

# The encoding for the databases
sql_engine_encoding = utf-8

# Collation for sqlite
sqlite_native_narrowing = False

# If SqlAlchemy should pool database connections.
sql_alchemy_pool_enabled = True

# The max number of db connections the pool will use.
sql_alchemy_pool_size = 5

# The maximum number of seconds each connection is allowed to be idle in the pool.
sql_alchemy_pool_recycle = 1800

# The maximum burst size for the pool's overflow if supported (ignored if not supported by driver).
sql_alchemy_max_overflow = 10

# The SQL mode
# sql_mode = PIPES_AS_CONCAT

# The schema that stores the airflow metadata.
# Set to the desired schema, leave blank for the default.
# sql_alchemy_schema =

# Import path for connect args if you want to extend the args airflow pass to sqlalchemy engine.
connect_args =

[logging]
# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = G:\Diabetes Risk Prediction System\src\airflow\logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

# Logging level
logging_level = INFO

# Logging level for Flask-appbuilder
fab_logging_level = WARNING

# Logging configuration
# By default, Airflow uses the default logging config of Python which writes stderr and rotates per 1GB.
# If you want to customize the logging (e.g., write logs to a file or use a different formatter),
# you can provide a custom logging configuration in a dictionary.
# The dictionary must contain a key for "version" with a value of 1.
# See https://docs.python.org/3/library/logging.config.html#configuration-dictionary-schema for details.
# Example custom logging configuration:
# logging_config =
#     {"version": 1,
#      "disable_existing_loggers": false,
#      "formatters": {
#          "airflow": {
#              "format": "%(asctime)s %(levelname)s - %(name)s - %(message)s",
#              }}, "
#      "handlers": {
#          "console": {
#             "class": "logging.StreamHandler",
#             "formatter": "airflow",
#             "stream": "ext://sys.stdout"
#             }},
#      "root": {
#          "level": "INFO",
#          "handlers": ["console"]
#      }}

# Logging levels for each of the built-in loggers
# Valid options are INFO, DEBUG, WARNING, ERROR, CRITICAL
logging_config_class =

# Setting this to False disables the default logging handler on the root logger
# This prevents double logging when using custom logging configs
enable_standard_error_logging = True
